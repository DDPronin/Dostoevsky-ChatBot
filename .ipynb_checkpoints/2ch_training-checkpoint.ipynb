{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf637583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf32039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e909b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/dialogues.jsonl\", lines=True)\n",
    "df[\"dialogue\"] = df[\"dialogue\"].apply(lambda x: x[:2])\n",
    "df[\"reply\"] = df[\"dialogue\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b4c7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\- да не было такого! я даже ни одной парочки у нас в школе не знаю! руки леры почти по плечо в мыле, она драит их так яростно, что кожа уже покраснела.',\n",
       " '→ \\\\- может быть, ты шпионишь за парочками, а потом собираешь их, когда они заканчивают свои дела! кота выдал самую очевидную и логичную мысль.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09f5169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'асуи сильная девочка, хоть она и небольшая. она всегда представляла себя как ту мышь с иголкой, из сказки \"алиса в стране чудес\", которая была тоже очень маленькой, но отважной и, поэтому, сильной. девушка решила проявить никчёмное, в данный момент, мужество: \\\\- н-нет, помощь не нужна. я смогу дойти сама… но компания мне, всё таки, нужна! так что пойдём. пара вышла из класса и отправилась в медпункт через школьную коридоры. по пути, они проходили мимо дамской комнаты, глянув на которую, асуи остановила мишу и кивнула головой на дверь: \\\\- я столько кофе сегодня выпила утром, ты бы знала. и сейчас мне придётся отвечать. пойдёшь со мной?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[9]['reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edf11c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[— я бы прошелся до крыши, — сказал он, встава...</td>\n",
       "      <td>— я бы прошелся до крыши, — сказал он, вставая...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[\\- да не было такого! я даже ни одной парочки...</td>\n",
       "      <td>\\- да не было такого! я даже ни одной парочки ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[девушка стала следовать за хаккури и вскоре о...</td>\n",
       "      <td>девушка стала следовать за хаккури и вскоре он...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[&gt;сама дойдешь? в смысле не нужно ли ее поддер...</td>\n",
       "      <td>&gt;сама дойдешь? в смысле не нужно ли ее поддерж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[вода на секунду приобрела зеленый оттенок. по...</td>\n",
       "      <td>вода на секунду приобрела зеленый оттенок. пок...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[блять, кота, как персонаж ты меня жутко бесиш...</td>\n",
       "      <td>блять, кота, как персонаж ты меня жутко бесишь...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[поняв наконец, о чем шла речь, миша сказала —...</td>\n",
       "      <td>поняв наконец, о чем шла речь, миша сказала — ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[так он типа сидит \\- ты что, прохуёбина, взду...</td>\n",
       "      <td>так он типа сидит \\- ты что, прохуёбина, вздум...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[почему бесит? ты кто вообще?, блять, кота, ка...</td>\n",
       "      <td>почему бесит? ты кто вообще?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>268332378</td>\n",
       "      <td>[девушка передаёт свой чистый комплект одежды ...</td>\n",
       "      <td>девушка передаёт свой чистый комплект одежды п...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           dialogue  \\\n",
       "0   268332378  [— я бы прошелся до крыши, — сказал он, встава...   \n",
       "1   268332378  [\\- да не было такого! я даже ни одной парочки...   \n",
       "2   268332378  [девушка стала следовать за хаккури и вскоре о...   \n",
       "3   268332378  [>сама дойдешь? в смысле не нужно ли ее поддер...   \n",
       "4   268332378  [вода на секунду приобрела зеленый оттенок. по...   \n",
       "..        ...                                                ...   \n",
       "65  268332378  [блять, кота, как персонаж ты меня жутко бесиш...   \n",
       "66  268332378  [поняв наконец, о чем шла речь, миша сказала —...   \n",
       "67  268332378  [так он типа сидит \\- ты что, прохуёбина, взду...   \n",
       "68  268332378  [почему бесит? ты кто вообще?, блять, кота, ка...   \n",
       "69  268332378  [девушка передаёт свой чистый комплект одежды ...   \n",
       "\n",
       "                                                reply  \n",
       "0   — я бы прошелся до крыши, — сказал он, вставая...  \n",
       "1   \\- да не было такого! я даже ни одной парочки ...  \n",
       "2   девушка стала следовать за хаккури и вскоре он...  \n",
       "3   >сама дойдешь? в смысле не нужно ли ее поддерж...  \n",
       "4   вода на секунду приобрела зеленый оттенок. пок...  \n",
       "..                                                ...  \n",
       "65  блять, кота, как персонаж ты меня жутко бесишь...  \n",
       "66  поняв наконец, о чем шла речь, миша сказала — ...  \n",
       "67  так он типа сидит \\- ты что, прохуёбина, вздум...  \n",
       "68                       почему бесит? ты кто вообще?  \n",
       "69  девушка передаёт свой чистый комплект одежды п...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f16ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71e39ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543840af34aa4c92bdf889f1a45c202d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/rugpt3medium_based_on_gpt2')\n",
    "model = AutoModelWithLMHead.from_pretrained('ai-forever/rugpt3medium_based_on_gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8973a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_param(text: str) -> str:\n",
    "    tokens_count = len(tokenizer.encode(text))\n",
    "    if tokens_count <= 15:\n",
    "        len_param = '1'\n",
    "    elif tokens_count <= 50:\n",
    "        len_param = '2'\n",
    "    elif tokens_count <= 256:\n",
    "        len_param = '3'\n",
    "    else:\n",
    "        len_param = '-'\n",
    "    return len_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8dcf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DvachDataset:\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        prefix = self.df.iloc[i][\"dialogue\"][1]\n",
    "        text = self.df.iloc[i][\"dialogue\"][0]\n",
    "        item = self.tokenizer(f\"|0|{get_length_param(prefix)}|\" + prefix + tokenizer.eos_token +  f\"|1|{get_length_param(text)}|\" + text, max_length=512, padding='max_length', truncation=True)\n",
    "        item[\"labels\"] = item[\"input_ids\"].copy()\n",
    "        return item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee94da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = train_test_split(df, test_size=0.2)\n",
    "train_dataset = DvachDataset(train_df, tokenizer)\n",
    "eval_dataset = DvachDataset(eval_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2439ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd7bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer, max_length=512, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d2da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62ccb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = \"2ch_training\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_train_epochs = 1,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 1,\n",
    "    #fp16 = True,\n",
    "    eval_steps = 1,\n",
    "    dataloader_num_workers = 4,\n",
    "    label_names = [\"input_ids\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 56\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 355871744\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"2ch_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"нужно допинать витю, который почему-то 24/7 охуенно занят, хотя при этом ничего не делает, чтобы записать\"\n",
    "test_input = test_input + \" Что скажешь?\" + tokenizer.eos_token +  \"|1|2|\"\n",
    "\n",
    "input_ids = tokenizer([test_input], return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenizer.decode(model.generate(input_ids.cuda(),\n",
    "                                max_length=len(tokenizer([test_input], return_tensors=\"pt\").input_ids[0]) + 32,\n",
    "                                bad_words_ids=[[tokenizer.pad_token_id]],\n",
    "                                force_words_ids=[[11649], [11649]],\n",
    "                                temperature=1.,\n",
    "                                repetition_penalty=10.,\n",
    "                                do_sample=True).cpu()[:, input_ids.shape[-1]:][0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d96393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15e817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78515288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9fe032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_tokenizer_and_model(model=\"tinkoff-ai/ruDialoGPT-medium\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load tokenizer and model instance for some specific DialoGPT model.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer and model\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, padding_side='left')\n",
    "    model = AutoModelForCausalLM.from_pretrained(model)\n",
    "  \n",
    "    # Return tokenizer and model\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a response to some user input.\n",
    "    \"\"\"\n",
    "  # Encode user input and End-of-String (EOS) token\n",
    "    new_input_ids = tokenizer.encode(input(\">> You:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "  # Append tokens to chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round > 0 else new_input_ids\n",
    "\n",
    "  # Generate response given maximum chat length history of 1250 tokens\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
    "  \n",
    "  # Print response\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "  \n",
    "  # Return the chat history ids\n",
    "    return chat_history_ids\n",
    "\n",
    "\n",
    "def chat_for_n_rounds(n=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Chat with chatbot for n rounds (n = 5 by default)\n",
    "    \"\"\"\n",
    "  \n",
    "  # Initialize tokenizer and model\n",
    "    tokenizer, model = load_tokenizer_and_model()\n",
    "  \n",
    "  # Initialize history variable\n",
    "    chat_history_ids = None\n",
    "  \n",
    "  # Chat for n rounds\n",
    "    for chat_round in range(n):\n",
    "        chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5632566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "model_name = 'ai-forever/rugpt3medium_based_on_gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d56c614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ты кто?\n",
      "\n",
      "— Я, — ответил я. – Я не знаю, кто вы, но я вас не боюсь. А вот вы меня не боитесь. И я вам не завидую.\n",
      "Ты кто?\n",
      "\n",
      "— Я — это я, — сказал я. – А ты кто такой? Я тебя раньше не видел. Как тебя зовут? Где ты живешь? Что ты здесь делаешь?\n",
      "Ты кто?\n",
      "\n",
      "– Я, – ответил я. — А ты кто такой? Я тебя не знаю. Откуда ты знаешь, что я – это я, а не кто-то другой?\n",
      "Ты кто?\n",
      "\n",
      "— Я? — удивился он. – Я — это я. Ты что, не узнал меня? Ну, конечно, узнал. Я же сказал тебе, что я — твой\n",
      "Ты кто?\n",
      "\n",
      "– Я, – ответил я. — А ты кто такой? И откуда знаешь, что я – это я, а не кто-нибудь другой? Я тебя не знаю.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('Ты кто?', return_tensors='pt')\n",
    "generated_token_ids = model.generate(\n",
    "    **inputs,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=5,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=1.2,\n",
    "    repetition_penalty=1.2,\n",
    "    length_penalty=1.0,\n",
    "    eos_token_id=50257,\n",
    "    max_new_tokens=40\n",
    ")\n",
    "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
    "for element in context_with_response:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24232f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dea3dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы: Как ты?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Как ты? – спросил он.\n",
      "\n",
      "Я покачала головой, стараясь не показывать своих страданий и боли в груди: это все равно было невыносимо! Как я могла рассказать ему о том, что произошло ночью?.. Но\n"
     ]
    }
   ],
   "source": [
    "start = input('Вы: ')\n",
    "inputs = tokenizer(start, return_tensors='pt')\n",
    "generated_token_ids = model.generate(\n",
    "    **inputs,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=1.2,\n",
    "    repetition_penalty=1.2,\n",
    "    length_penalty=1.0,\n",
    "    eos_token_id=50257,\n",
    "    max_new_tokens=40\n",
    ")\n",
    "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
    "print(context_with_response[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = []\n",
    "for i in range(3):\n",
    "    human = input('Вы: ')\n",
    "    context.append(human)\n",
    "    inputs = tokenizer(' '.join(context)+'.', return_tensors='pt')\n",
    "    generated_token_ids = model.generate(\n",
    "        **inputs,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        num_beams=1,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=1.2,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.2,\n",
    "        eos_token_id=50257,\n",
    "        max_new_tokens=40\n",
    "    )\n",
    "    context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
    "    #print('ruGPT-3-medium: ', context_with_response)\n",
    "    answer = tokenizer.decode(generated_token_ids[0]).replace(' '.join(context)+'.', '')\n",
    "    context.append(answer)\n",
    "    print('ruGPT-3-medium: ', answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aaeabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
